{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of training results and visualization of YOLO predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, List, Dict, Optional, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.metrics import DetMetrics\n",
    "\n",
    "from inference import create_filepath_inference_lists, Yolo3dBatchInference\n",
    "\n",
    "from viz import (\n",
    "    plot_images_with_bboxes,\n",
    "    png_to_np,\n",
    "    bbox_txt_to_list,\n",
    "    unnormalize_yolo_bboxes,\n",
    ")\n",
    "from utils import (\n",
    "    analyze_best_metrics,\n",
    "    MAP50_COL,\n",
    "    MAP50_95_COL,\n",
    "    PRECISION_COL,\n",
    "    RECALL_COL,\n",
    "    FITNESS_COL,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICE_NAMES: Dict[int, str] = {0: \"sagittal\", 1: \"coronal\", 2: \"axial\"}\n",
    "MODALITIES: List[str] = [\"FLAIR\", \"T1CE\", \"T2\", \"PD\", \"T1\", \"gt\"]\n",
    "\n",
    "# Path to raw .nii.gz. nifti files\n",
    "BASE_DATASET_PATH = Path(\"/path/to/your/data/please/edit/me\")\n",
    "RAW_MS_SHIFT_DATASET_PATH = BASE_DATASET_PATH / \"MS_shift\"\n",
    "RAW_MSLESSEG_DATASET_PAT = BASE_DATASET_PATH / \"MSLesSeg\"\n",
    "\n",
    "#  We moved 1-20 test patients to train split.\n",
    "#  And 21-23 test patients to val split in MS_Shift dataset\n",
    "#  Thats why in test we had only 10 patients (24-33)\n",
    "TEST_SPLIT_INDEXES_MS_SHIFT = tuple(range(24, 34))\n",
    "\n",
    "# Path to processed files (2d .png images)\n",
    "PROCESSED_MS_SHIFT_DATASET_PATH = Path(\n",
    "    \"/path/to/your/data/please/edit/me\"\n",
    ")\n",
    "PROCESSED_MS_SHIFT_MSLESSEG_DATASET_PATH = Path(\n",
    "    \"/path/to/your/data/please/edit/me\"\n",
    ")\n",
    "\n",
    "LABELS_DIR = \"labels\"\n",
    "IMAGES_DIR = \"images\"\n",
    "\n",
    "# Paths to trained models\n",
    "RUNS_DIR = \"/path/to/your/data/please/edit/me/runs/detect\"\n",
    "\n",
    "\"\"\"\n",
    "Model Naming Convention Specification for MS Lesion Detection YOLO Models\n",
    "\n",
    "**Format:**\n",
    "{DATASET_CODES}_{MODALITIES}_{SLICE_DIMS}_{EPOCHS}ep_{CLASSES}cls_{IMGSZ}imgsz_{AUGMENT}_{MODEL_SIZE}{YOLO_VERSION}_{COMMENTS}\n",
    "\n",
    "1.  **{DATASET_CODES}** (Required)\n",
    "    - Code(s) indicating the dataset(s) used.\n",
    "    - Codes: 'mss' (MS Shift), 'msl' (MSLesSeg).\n",
    "    - Combined: 'msl_mss' or 'mss_msl'.\n",
    "\n",
    "2.  **{MODALITIES}** (Optional - specify only if **not** using all modalities)\n",
    "    - MRI modality subset used.\n",
    "    - Codes: 'FLAIR', 'T1', 'T2', 'PD', 'T1CE'.\n",
    "    - Example: 'FLAIR'.\n",
    "\n",
    "3.  **{SLICE_DIMS}'** (Optional - specify only if **not** using all projections)\n",
    "    - Slice projections used for 2D data generation.\n",
    "    - Codes: 'sag' (sagittal, dim=0), 'cor' (coronal, dim=1), 'ax' (axial, dim=2).\n",
    "    - Example: 'ax'.\n",
    "\n",
    "4.  **{EPOCHS}ep** (Required)\n",
    "    - Number of training epochs.\n",
    "    - Format: number + 'ep'.\n",
    "    - Example: '100ep', '50ep'\n",
    "\n",
    "5.  **{CLASSES}cls** (Optional. Default is 1 cls)\n",
    "    - Number of detection classes.\n",
    "    - Format: number + 'cls'.\n",
    "    - Example: '1cls', '2cls'\n",
    "\n",
    "6.  **{IMGSZ}imgsz** (Required)\n",
    "    - Square image size used during training.\n",
    "    - Format: number + 'imgsz'.\n",
    "    - Example: '640imgsz', '256imgsz'\n",
    "\n",
    "7.  **{AUGMENT}** (Optional - specify **if** augmentation was enabled)\n",
    "    - Flag indicating use of standard Ultralytics augmentations.\n",
    "    - Code: 'aug'\n",
    "    - Example: 'aug'\n",
    "\n",
    "8.  **{MODEL_SIZE}** (Required)\n",
    "    - Base YOLO model size/architecture.\n",
    "    - Codes: 'n', 's', 'm', 'l', 'x'.\n",
    "    - Example: 'l', 'm'\n",
    "\n",
    "9.  **{YOLO_VERSION}** (Required)\n",
    "    - YOLO architecture version if different from standard (e.g., v8) or custom.\n",
    "    - Format: number\n",
    "    - Example: '11'\n",
    "\n",
    "10.  **{COMMENTS}** (Optional)\n",
    "    - Other parameters\n",
    "    - Example: 'filtered data'\n",
    "\"\"\"\n",
    "# Commented models wat trained on different sizes of train test val:\n",
    "# Other models trained, tested and valuated on a same sizes\n",
    "MODEL_DIRS_NAMES_PATHS = {\n",
    "    \"mss_23ep_640imgsz_l12\": \"MS_shift/train_All_mods_dims_23ep_1cls_640imgsz_Yolo12_large\",\n",
    "}\n",
    "\n",
    "BEST_MODEL_PATHS = {\n",
    "    model_name: os.path.join(RUNS_DIR, model_path, \"weights/best.pt\")\n",
    "    for model_name, model_path in MODEL_DIRS_NAMES_PATHS.items()\n",
    "}\n",
    "\n",
    "DATA_YAML_MS_SHIFT = \"MS_Shift.yaml\"\n",
    "DATA_YAML_MSSHIFT_MSLESSEG = \"MSShift_MSLesSeg.yaml\"\n",
    "\n",
    "# Images to render (specify relative paths from IMAGES_DIR)\n",
    "IMAGES_TO_VISUALIZE: List[str] = [\n",
    "    \"test/MSShift_test_24_FLAIR_idx_169_axial.png\",\n",
    "    \"test/MSShift_test_28_FLAIR_idx_89_axial.png\",\n",
    "    # \"test/MSShift_test_25_FLAIR_idx_89_axial.png\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_models(\n",
    "    model_paths_dict: Dict[str, str],\n",
    "    model_keys_to_load: Optional[Union[str, List[str]]] = None,\n",
    ") -> Dict[str, YOLO]:\n",
    "    \"\"\"\n",
    "        Loads one or more YOLO models from a path dictionary.\n",
    "        Checks for file existence before loading and handles errors.\n",
    "\n",
    "    Args:\n",
    "        model_paths_dict (Dict[str, str]): Dictionary where keys are model names (strings)\n",
    "                                           and values ​​are paths to .pt weight files (strings).\n",
    "        model_keys_to_load (Optional[Union[str, List[str]]], optional):\n",
    "            - If None (default): Loads all models from model_paths_dict.\n",
    "            - If string: Loads only the model with the specified key.\n",
    "            - If list of strings: Loads only the models with the keys in the list.\n",
    "            This saves GPU memory by loading only the models you need.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, YOLO]: A dictionary containing loaded YOLO objects,\n",
    "                             where the keys are the names of the models that were successfully loaded.\n",
    "    \"\"\"\n",
    "    loaded_models: Dict[str, YOLO] = {}\n",
    "    keys_to_process: List[str] = []\n",
    "\n",
    "    # Determine which keys (models) need to be loaded\n",
    "    if model_keys_to_load is None:\n",
    "        keys_to_process = list(model_paths_dict.keys())\n",
    "        logger.info(f\"Attempting to load all {len(keys_to_process)} models specified.\")\n",
    "    elif isinstance(model_keys_to_load, str):\n",
    "        if model_keys_to_load in model_paths_dict:\n",
    "            keys_to_process = [model_keys_to_load]\n",
    "            logger.info(f\"Attempting to load specified model: '{model_keys_to_load}'\")\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"Specified model key '{model_keys_to_load}' not found in model_paths_dict. No models will be loaded.\"\n",
    "            )\n",
    "            return loaded_models  # Return an empty dictionary\n",
    "    elif isinstance(model_keys_to_load, list):\n",
    "        keys_to_process = [key for key in model_keys_to_load if key in model_paths_dict]\n",
    "        if len(keys_to_process) < len(model_keys_to_load):\n",
    "            missing_keys = set(model_keys_to_load) - set(keys_to_process)\n",
    "            logger.warning(\n",
    "                f\"Specified model keys not found in model_paths_dict: {missing_keys}. Loading only found models.\"\n",
    "            )\n",
    "        if not keys_to_process:\n",
    "            logger.warning(\n",
    "                \"None of the specified model keys were found. No models will be loaded.\"\n",
    "            )\n",
    "            return loaded_models\n",
    "        logger.info(f\"Attempting to load specified models: {keys_to_process}\")\n",
    "    else:\n",
    "        logger.error(\n",
    "            f\"Invalid type for model_keys_to_load: {type(model_keys_to_load)}. Expected None, str, or List[str].\"\n",
    "        )\n",
    "        return loaded_models  # Return an empty dictionary\n",
    "\n",
    "    # Loading selected models\n",
    "    for name in keys_to_process:\n",
    "        path_str = model_paths_dict[name].strip()\n",
    "        path = Path(path_str)\n",
    "\n",
    "        if path.is_file():\n",
    "            try:\n",
    "                loaded_models[name] = YOLO(str(path))  # Use str(path) for compatibility\n",
    "                logger.info(f\"Loaded model '{name}' from {path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    f\"Error loading model '{name}' from {path}: {e}\", exc_info=True\n",
    "                )\n",
    "        else:\n",
    "            logger.warning(f\"Skipping model '{name}' - file not found at {path}.\")\n",
    "\n",
    "    logger.info(f\"Loaded {len(loaded_models)} models.\")\n",
    "    return loaded_models\n",
    "\n",
    "\n",
    "def extract_imgsz_from_name(model_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Extracts imgsize from model name. Used in YOLO inference.py\n",
    "\n",
    "    Args:\n",
    "        model name (str): model name (keys in MODEL_DIRS_NAMES_PATHS)\n",
    "    \"\"\"\n",
    "    imgsz = 640  # default\n",
    "    if \"256imgsz\" in model_name:\n",
    "        imgsz = 256\n",
    "    elif \"224imgsz\" in model_name:\n",
    "        imgsz = 224\n",
    "    return imgsz\n",
    "\n",
    "\n",
    "def extract_serializable_metrics(\n",
    "    metrics_obj: Optional[DetMetrics],\n",
    ") -> Optional[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Extracts the primary numeric metrics from the DetMetrics object for storing in JSON.\n",
    "\n",
    "    Args:\n",
    "        metrics_obj (Optional[DetMetrics]): The metrics object from model.val().\n",
    "\n",
    "    Returns:\n",
    "        Optional[Dict[str, float]]: A dictionary of primary metrics, or None if the input object is invalid.\n",
    "    \"\"\"\n",
    "    if metrics_obj is None or not hasattr(metrics_obj, \"box\"):\n",
    "        return None\n",
    "\n",
    "    output_metrics: Dict[str, Any] = {}\n",
    "    # Extracting box metrics\n",
    "    if hasattr(metrics_obj, \"box\"):\n",
    "        try:\n",
    "            box_metrics = metrics_obj.box\n",
    "            output_metrics.update(\n",
    "                {\n",
    "                    \"map50\": float((getattr(box_metrics, \"map50\", None))),\n",
    "                    \"map75\": float((getattr(box_metrics, \"map75\", None))),\n",
    "                    \"map50-95\": float((getattr(box_metrics, \"map\", None))),\n",
    "                    \"precision\": float((getattr(box_metrics, \"p\", None))),\n",
    "                    \"recall\": float((getattr(box_metrics, \"r\", None))),\n",
    "                    \"mean_precision\": float(\n",
    "                        (getattr(box_metrics, \"mp\", None))\n",
    "                    ),  # Mean Precision\n",
    "                    \"mean_recall\": float(\n",
    "                        (getattr(box_metrics, \"mr\", None))\n",
    "                    ),  # Mean Recall\n",
    "                    \"mean_f1\": float((getattr(box_metrics, \"f1\", None))),\n",
    "                    \"fitness\": float((getattr(metrics_obj, \"fitness\", None))),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting serializable metrics: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    if (\n",
    "        hasattr(metrics_obj, \"confusion_matrix\")\n",
    "        and metrics_obj.confusion_matrix is not None\n",
    "    ):\n",
    "        cm = metrics_obj.confusion_matrix\n",
    "        if hasattr(cm, \"matrix\") and isinstance(cm.matrix, np.ndarray):\n",
    "            # Convert numpy array to list of lists for JSON serialization\n",
    "            output_metrics[\"confusion_matrix\"] = cm.matrix.tolist()\n",
    "            tp = cm.matrix[0, 0]\n",
    "            fn = cm.matrix[1, 0]\n",
    "            output_metrics[\"images_scanned\"] = int(tp + fn)\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Metrics object has 'confusion_matrix' attribute, but no valid '.matrix' numpy array found.\"\n",
    "        )\n",
    "\n",
    "    return output_metrics\n",
    "\n",
    "\n",
    "def save_metrics_to_json(metrics_dict: Dict[str, Any], filepath: Union[str, Path]):\n",
    "    \"\"\"\n",
    "    Saves a dictionary of metrics to a JSON file.\n",
    "    Overwrites the file if it exists.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict (Dict[str, Any]): Dictionary of metrics to save.\n",
    "                                       Values ​​are assumed to be serializable to JSON.\n",
    "        filepath (Union[str, Path]): Path to file to save the JSON to.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = Path(filepath)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metrics_dict, f, indent=4, ensure_ascii=False)\n",
    "        logger.info(f\"Metrics successfully saved to: {path}\")\n",
    "    except TypeError as te:\n",
    "        logger.error(\n",
    "            f\"Failed to serialize metrics to JSON. Ensure all values are JSON-serializable (e.g., numbers, strings, lists, dicts): {te}\",\n",
    "            exc_info=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Failed to save metrics summary to {filepath}: {e}\", exc_info=True\n",
    "        )\n",
    "\n",
    "\n",
    "def load_metrics_from_json(filepath: Union[str, Path]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Loads a dictionary of metrics from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        filepath (Union[str, Path]): Path to the JSON file with metrics.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Dict[str, Any]]: The loaded dictionary of metrics, or None if the file\n",
    "        was not found or a read/parse error occurred.\n",
    "    \"\"\"\n",
    "    path = Path(filepath)\n",
    "    if not path.is_file():\n",
    "        logger.warning(f\"Metrics file not found: {path}. Returning None.\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_metrics = json.load(f)\n",
    "        logger.info(f\"Metrics successfully loaded from: {path}\")\n",
    "        return loaded_metrics\n",
    "    except json.JSONDecodeError as jde:\n",
    "        logger.error(f\"Error decoding JSON from {path}: {jde}\", exc_info=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load metrics summary from {path}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def display_metrics_summary(metrics_summary: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of metrics to a DataFrame and outputs it.\n",
    "\n",
    "        Args:\n",
    "            metrics_summary (Dict[str, Any]): Dictionary loaded from JSON or assembled in the process.\n",
    "            Expected structure: {model_name: {split_name: {metric: value}}}\n",
    "    \"\"\"\n",
    "    if not metrics_summary:\n",
    "        print(\"Metrics summary is empty. Nothing to display.\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for model_name, splits_data in metrics_summary.items():\n",
    "        for split_name, metrics_data in splits_data.items():\n",
    "            row = {\"model\": model_name, \"split\": split_name}\n",
    "            if metrics_data:\n",
    "                row.update(metrics_data)\n",
    "            else:\n",
    "                # Add NaN for models/splits with errors or without metrics\n",
    "                row.update(\n",
    "                    {\n",
    "                        m: np.nan\n",
    "                        for m in [\"map\", \"map50\", \"map75\", \"precision\", \"recall\", \"f1\"]\n",
    "                    }\n",
    "                )\n",
    "            rows.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    print(\"\\n--- Validation Metrics Summary ---\")\n",
    "    # Use to_string to output the entire table without truncation\n",
    "    print(summary_df.to_string(float_format=\"%.4f\", na_rep=\"N/A\"))\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "def validate_and_print_detailed_metrics(\n",
    "    model: YOLO,\n",
    "    model_name: str,\n",
    "    data_yaml: str,\n",
    "    splits: Optional[List[str]] = None,\n",
    "    imgsz: int = 640,\n",
    "    batch: int = 24,\n",
    "    iou_threshold: Optional[float] = None,\n",
    "    conf_threshold: Optional[float] = None,\n",
    ") -> Dict[str, Optional[DetMetrics]]:\n",
    "    \"\"\"\n",
    "    Runs YOLO model validation on the specified splits and outputs detailed metrics.\n",
    "\n",
    "    Args:\n",
    "        model (YOLO): The loaded Ultralytics YOLO model object.\n",
    "        model_name (str): The model name for logging and headers.\n",
    "        data_yaml (str): Path to the data.yaml file describing the dataset.\n",
    "        splits (Optional[List[str]], optional): List of splits to validate (e.g., ['val', 'test']).\n",
    "        If None, 'val' is used. Defaults to None.\n",
    "        imgsz (int, optional): Image size to validate. Defaults to 640.\n",
    "        batch (int, optional): Batch size to validate. Defaults to 32.\n",
    "        iou_threshold (Optional[float], optional): IoU threshold for NMS. If None, YOLO default (0.6) is used.\n",
    "        conf_threshold (Optional[float], optional): Confidence threshold for calculating metrics. If None, the default YOLO (0.001) is used.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Optional[DetMetrics]]: A dictionary where keys are split names\n",
    "                                        and values ​​are DetMetrics objects with validation results or None on error.\n",
    "    \"\"\"\n",
    "    if splits is None:\n",
    "        splits = [\"val\"]\n",
    "\n",
    "    if not Path(data_yaml).is_file():\n",
    "        logger.error(f\"Data YAML file not found: {data_yaml}\")\n",
    "        return {split: None for split in splits}\n",
    "\n",
    "    logger.info(f\"--- Starting Validation for Model: {model_name} ---\")\n",
    "    logger.info(f\"  Dataset: {data_yaml}\")\n",
    "    logger.info(f\"  Splits: {splits}\")\n",
    "    logger.info(f\"  Image Size: {imgsz}\")\n",
    "    logger.info(f\"  Batch Size: {batch}\")\n",
    "    if iou_threshold is not None:\n",
    "        logger.info(f\"  IoU Threshold: {iou_threshold}\")\n",
    "    if conf_threshold is not None:\n",
    "        logger.info(f\"  Conf Threshold: {conf_threshold}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    results_dict: Dict[str, Optional[DetMetrics]] = {}\n",
    "\n",
    "    for split in splits:\n",
    "        logger.info(f\"Validating on split: '{split}'...\")\n",
    "        metrics = None\n",
    "        run_name = f\"{model_name}_{split}\"\n",
    "        try:\n",
    "            # Create a dictionary of arguments for model.val to pass only non-None values\n",
    "            val_args = {\n",
    "                \"data\": data_yaml,\n",
    "                \"split\": split,\n",
    "                \"imgsz\": imgsz,\n",
    "                \"batch\": batch,\n",
    "                \"name\": run_name,\n",
    "                \"verbose\": True,  # Enable YOLO output for tracking\n",
    "            }\n",
    "            if iou_threshold is not None:\n",
    "                val_args[\"iou\"] = iou_threshold\n",
    "            if conf_threshold is not None:\n",
    "                val_args[\"conf\"] = conf_threshold\n",
    "\n",
    "            metrics = model.val(**val_args)\n",
    "\n",
    "            results_dict[split] = metrics\n",
    "            # --- Print Metrics ---\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"\\n--- Metrics for Model '{model_name}' on Split '{split}' ---\")\n",
    "            for key, value in extract_serializable_metrics(metrics).items():\n",
    "                if isinstance(value, (list, tuple)):\n",
    "                    formatted_value = str(value)  # Сonvert the list to a string\n",
    "                else:\n",
    "                    formatted_value = f\"{float(value):>10.4f}\"  # Formatting numbers\n",
    "                print(f\"  {key:<20}    {formatted_value}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"An error occurred during validation for split '{split}': {e}\",\n",
    "                exc_info=True,\n",
    "            )\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the number of labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_lesion_labels_amount(labels_dirpath: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the total number of lines (labels) in all .txt files\n",
    "    in the specified directory (e.g. train, val, or test).\n",
    "\n",
    "    Args:\n",
    "    labels_dirpath (str): Path to the directory containing the labels (.txt) files.\n",
    "\n",
    "    Returns:\n",
    "    int: Total number of lines (labels) found. Returns 0 if\n",
    "    the directory is not found or does not contain files.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    path = Path(labels_dirpath)\n",
    "    if not path.is_dir():\n",
    "        logger.warning(f\"Labels directory not found: {labels_dirpath}\")\n",
    "        return 0\n",
    "\n",
    "    label_files = list(path.glob(\"*.txt\"))\n",
    "    if not label_files:\n",
    "        logger.warning(f\"No label files (.txt) found in {labels_dirpath}\")\n",
    "        return 0\n",
    "\n",
    "    logger.info(f\"Counting labels in {len(label_files)} files in {labels_dirpath}...\")\n",
    "    for filepath in tqdm(label_files, desc=\"Counting labels\"):\n",
    "        try:\n",
    "            with open(filepath, \"r\") as f:\n",
    "                for line in f:\n",
    "                    if line.strip():  # Считаем непустые строки\n",
    "                        total += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file {filepath}: {e}\")\n",
    "    logger.info(f\"Total labels found: {total}\")\n",
    "    return total\n",
    "\n",
    "\n",
    "train_labels_count_MS_SHIFT = total_lesion_labels_amount(\n",
    "    str(PROCESSED_MS_SHIFT_DATASET_PATH / LABELS_DIR / \"train\")\n",
    ")\n",
    "val_labels_count_MS_SHIFT = total_lesion_labels_amount(\n",
    "    str(PROCESSED_MS_SHIFT_DATASET_PATH / LABELS_DIR / \"val\")\n",
    ")\n",
    "test_labels_count_MS_SHIFT = total_lesion_labels_amount(\n",
    "    str(PROCESSED_MS_SHIFT_DATASET_PATH / LABELS_DIR / \"test\")\n",
    ")\n",
    "\n",
    "print(f\"Total labels in train set: {train_labels_count_MS_SHIFT}\")\n",
    "print(f\"Total labels in validation set: {val_labels_count_MS_SHIFT}\")\n",
    "print(f\"Total labels in test set: {test_labels_count_MS_SHIFT}\")\n",
    "\n",
    "train_labels_count_MSShift_MSLesSeg = total_lesion_labels_amount(\n",
    "    str(PROCESSED_MS_SHIFT_MSLESSEG_DATASET_PATH / LABELS_DIR / \"train\")\n",
    ")\n",
    "val_labels_count_MSShift_MSLesSeg = total_lesion_labels_amount(\n",
    "    str(PROCESSED_MS_SHIFT_MSLESSEG_DATASET_PATH / LABELS_DIR / \"val\")\n",
    ")\n",
    "test_labels_count_MSShift_MSLesSeg = total_lesion_labels_amount(\n",
    "    str(PROCESSED_MS_SHIFT_MSLESSEG_DATASET_PATH / LABELS_DIR / \"test\")\n",
    ")\n",
    "\n",
    "print(f\"Total labels in train set: {train_labels_count_MSShift_MSLesSeg}\")\n",
    "print(f\"Total labels in validation set: {val_labels_count_MSShift_MSLesSeg}\")\n",
    "print(f\"Total labels in test set: {test_labels_count_MSShift_MSLesSeg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_yolo_models(BEST_MODEL_PATHS)\n",
    "\n",
    "dataset_to_use = PROCESSED_MS_SHIFT_DATASET_PATH\n",
    "# models_to_visualize = list(models.keys())\n",
    "models_to_visualize = [\n",
    "    \"mss_23ep_640imgsz_l12\",\n",
    "]\n",
    "\n",
    "for img_relative_path in IMAGES_TO_VISUALIZE:\n",
    "    img_full_path = dataset_to_use / IMAGES_DIR / img_relative_path\n",
    "    if not img_full_path.is_file():\n",
    "        logger.warning(\n",
    "            f\"Image file not found: {img_full_path}. Skipping visualization.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    logger.info(f\"\\n--- Generating predictions for: {img_relative_path} ---\")\n",
    "\n",
    "    predictions_list = []\n",
    "    titles_list = []\n",
    "\n",
    "    source_img_np = png_to_np(str(img_full_path))\n",
    "    if source_img_np is None:\n",
    "        logger.warning(\n",
    "            f\"Failed to load image array for {img_full_path}. Skipping visualization.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Get Ground Truth BBoxes once\n",
    "    gt_label_path = str(dataset_to_use / LABELS_DIR / img_relative_path).replace(\n",
    "        img_full_path.suffix, \".txt\"\n",
    "    )\n",
    "    gt_bboxes_yolo = bbox_txt_to_list(gt_label_path)\n",
    "    img_h, img_w = source_img_np.shape[:2]\n",
    "    gt_bboxes_pixels = unnormalize_yolo_bboxes(gt_bboxes_yolo, img_w, img_h)\n",
    "\n",
    "    images_for_plot = [source_img_np]\n",
    "    bboxes_for_plot = [gt_bboxes_pixels]\n",
    "    titles_for_plot = [\"Ground Truth\"]\n",
    "\n",
    "    for model_name in models_to_visualize:\n",
    "        if model_name in models:\n",
    "            model = models[model_name]\n",
    "            logger.info(f\"Predicting with model: {model_name}\")\n",
    "            try:\n",
    "                # Determine imgsz from the model name or use the default\n",
    "                imgsz_pred = extract_imgsz_from_name(model_name)\n",
    "\n",
    "                results = model.predict(\n",
    "                    str(img_full_path), imgsz=imgsz_pred, verbose=False\n",
    "                )\n",
    "                if results and results[0].boxes:\n",
    "                    pred_bboxes_yolo = results[0].boxes.xywhn.cpu().tolist()\n",
    "                    pred_bboxes_pixels = unnormalize_yolo_bboxes(\n",
    "                        pred_bboxes_yolo, img_w, img_h\n",
    "                    )\n",
    "                else:\n",
    "                    pred_bboxes_pixels = []\n",
    "\n",
    "                images_for_plot.append(source_img_np)\n",
    "                bboxes_for_plot.append(pred_bboxes_pixels)\n",
    "                titles_for_plot.append(model_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    f\"Error predicting with model {model_name} on {img_relative_path}: {e}\",\n",
    "                    exc_info=True,\n",
    "                )\n",
    "\n",
    "    if len(images_for_plot) > 1:\n",
    "        plot_images_with_bboxes(\n",
    "            images=images_for_plot,\n",
    "            bboxes_list=bboxes_for_plot,\n",
    "            titles=titles_for_plot,\n",
    "            main_title=f\"Predictions for: {img_relative_path}\",\n",
    "            cols=min(3, len(images_for_plot)),\n",
    "            figsize_scale=5,\n",
    "            fontsize=20,\n",
    "            # filename_to_save=f\"prediction_{Path(img_relative_path).stem}.png\"\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\n",
    "            f\"No successful predictions to visualize for {img_relative_path}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation and Metrics Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model.val() YOLO method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Configuration-----\n",
    "OUTPUT_METRICS_FILE = Path(\"validation_metrics_summary_correct.json\")\n",
    "SPLITS_TO_VALIDATE = [\"val\", \"test\"]\n",
    "BATCH_SIZE = 40\n",
    "\n",
    "# Load existing metrics if the file exists, or create an empty dictionary\n",
    "all_extracted_metrics = load_metrics_from_json(OUTPUT_METRICS_FILE) or {}\n",
    "\n",
    "logger.info(\"--- Starting/Resuming Sequential Validation ---\")\n",
    "\n",
    "# Select which models to validate now (you can use the entire BEST_MODEL_PATHS.keys())\n",
    "models_to_process_now = BEST_MODEL_PATHS.keys()\n",
    "# models_to_process_now = [\"mss_23ep_640imgsz_l12\"]\n",
    "\n",
    "for name in models_to_process_now:\n",
    "    if name not in BEST_MODEL_PATHS:\n",
    "        logger.warning(f\"Model key '{name}' not found in BEST_MODEL_PATHS. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Skip if metrics for this model already exist (optional)\n",
    "    if name in all_extracted_metrics and all(\n",
    "        all_extracted_metrics[name].get(s) for s in SPLITS_TO_VALIDATE\n",
    "    ):\n",
    "        logger.info(f\"Metrics for model '{name}' already exist. Skipping validation.\")\n",
    "        continue\n",
    "\n",
    "    logger.info(f\"Processing model: {name}\")\n",
    "    model_obj: Optional[YOLO] = None\n",
    "    model_results: Dict[str, Optional[DetMetrics]] = {}\n",
    "\n",
    "    try:\n",
    "        # 1. Load one model\n",
    "        loaded_dict = load_yolo_models(BEST_MODEL_PATHS, model_keys_to_load=name)\n",
    "        if name not in loaded_dict:\n",
    "            raise RuntimeError(f\"Failed to load model object for '{name}'\")\n",
    "        model_obj = loaded_dict[name]\n",
    "\n",
    "        # 2. Define the parameters\n",
    "        current_data_yaml = (\n",
    "            DATA_YAML_MSSHIFT_MSLESSEG if \"msl_mss\" in name else DATA_YAML_MS_SHIFT\n",
    "        )\n",
    "        current_imgsz = extract_imgsz_from_name(name)\n",
    "        current_batch = BATCH_SIZE\n",
    "\n",
    "        # 3. Perform validation\n",
    "        model_results = validate_and_print_detailed_metrics(\n",
    "            model=model_obj,\n",
    "            model_name=name,\n",
    "            data_yaml=current_data_yaml,\n",
    "            splits=SPLITS_TO_VALIDATE,\n",
    "            imgsz=current_imgsz,\n",
    "            batch=current_batch,\n",
    "        )\n",
    "\n",
    "        # 4. Extract and save metrics for the current model\n",
    "        extracted_metrics_for_model = {}\n",
    "        for split, metrics_obj in model_results.items():\n",
    "            extracted_metrics_for_model[split] = extract_serializable_metrics(\n",
    "                metrics_obj\n",
    "            )\n",
    "        all_extracted_metrics[name] = (\n",
    "            extracted_metrics_for_model  # Updating the common dictionary\n",
    "        )\n",
    "\n",
    "        # 5. Save the updated metrics dictionary to a file after each model\n",
    "        save_metrics_to_json(all_extracted_metrics, OUTPUT_METRICS_FILE)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during processing model '{name}': {e}\", exc_info=True)\n",
    "\n",
    "    finally:\n",
    "        # 6. Freeing up GPU memory\n",
    "        if model_obj is not None:\n",
    "            logger.info(f\"Unloading model '{name}' and clearing CUDA cache...\")\n",
    "            del model_obj\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                logger.info(\"CUDA cache cleared.\")\n",
    "            else:\n",
    "                logger.info(\"CUDA not available, skipping cache clear.\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "logger.info(\"--- Finished Sequential Validation Run ---\")\n",
    "\n",
    "# 7. Output the final table\n",
    "final_metrics = load_metrics_from_json(OUTPUT_METRICS_FILE)\n",
    "if final_metrics:\n",
    "    display_metrics_summary(final_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom inference methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_process = \"mss_23ep_640imgsz_l12\"\n",
    "current_model = load_yolo_models(\n",
    "    BEST_MODEL_PATHS, model_keys_to_load=models_to_process\n",
    ")[models_to_process]\n",
    "imgs_list, gts_list = create_filepath_inference_lists(\n",
    "    RAW_MS_SHIFT_DATASET_PATH, \"Test\", TEST_SPLIT_INDEXES_MS_SHIFT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inference_mss_23ep_640imgsz_l12 = Yolo3dBatchInference(\n",
    "    yolo_model=current_model,\n",
    "    nifti_filepaths=imgs_list,\n",
    "    gt_filepaths=gts_list,\n",
    "    conf=0.001,\n",
    "    iou=0.6,\n",
    "    slice_dims=[0, 1, 2],\n",
    ")\n",
    "batch_inference_mss_23ep_640imgsz_l12.run_batch_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inference_mss_23ep_640imgsz_l12.compute_aggregate_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing metrics from results.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FILENAME = \"results.csv\"\n",
    "PARENT_DIR = Path(\"/path/to/your/data/please/edit/me\")\n",
    "\n",
    "results_data: Dict[str, Dict[str, Optional[Union[pd.DataFrame, Dict]]]] = {}\n",
    "\n",
    "for model_name, run_dir in MODEL_DIRS_NAMES_PATHS.items():\n",
    "    results_file = PARENT_DIR / RUNS_DIR / Path(run_dir) / RESULTS_FILENAME\n",
    "    logger.info(f\"--- Analyzing results for: {model_name} ---\")\n",
    "    results_data[model_name] = {\"df\": None, \"analysis\": None}\n",
    "\n",
    "    # if results_file.is_file():\n",
    "    try:\n",
    "        df = pd.read_csv(results_file)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        results_data[model_name][\"df\"] = df\n",
    "        logger.info(f\"Loaded results from: {results_file}\")\n",
    "\n",
    "        analysis = analyze_best_metrics(\n",
    "            df.copy(),\n",
    "            map50_col=MAP50_COL,\n",
    "            map50_95_col=MAP50_95_COL,\n",
    "            precision_col=PRECISION_COL,\n",
    "            recall_col=RECALL_COL,\n",
    "            fitness_col=FITNESS_COL,\n",
    "        )\n",
    "        results_data[model_name][\"analysis\"] = analysis\n",
    "        logger.info(f\"Analysis complete for {model_name}.\")\n",
    "\n",
    "        print(f\"\\nAnalysis Summary for '{model_name}':\")\n",
    "        pprint(analysis)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Results file not found: {results_file}\")\n",
    "    except KeyError as e:\n",
    "        logger.error(\n",
    "            f\"Missing expected column in {results_file}: {e}. Analysis might be incomplete.\"\n",
    "        )\n",
    "        # Can print df.columns for debugging\n",
    "        if (\n",
    "            \"df\" in results_data[model_name]\n",
    "            and results_data[model_name][\"df\"] is not None\n",
    "        ):\n",
    "            logger.debug(\n",
    "                f\"Available columns: {list(results_data[model_name]['df'].columns)}\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process results for {model_name}: {e}\", exc_info=True)\n",
    "    # else:\n",
    "    #     logger.warning(f\"Results file not found: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of Models\n",
    "\n",
    "comparison_metrics = {}\n",
    "for model_name, data in results_data.items():\n",
    "    analysis = data.get(\"analysis\")\n",
    "    if analysis:\n",
    "        best_fitness_epoch, best_fitness_value = analysis.get(\n",
    "            \"best_fitness\", (None, None)\n",
    "        )\n",
    "        mAP50_at_best_fitness_epoch, mAP50_at_best_fitness = analysis.get(\n",
    "            \"best_fitness_epoch_mAP50\", (None, None)\n",
    "        )\n",
    "        mAP50_95_at_best_fitness_epoch, mAP50_95_at_best_fitness = analysis.get(\n",
    "            \"best_fitness_epoch_mAP50-95\", (None, None)\n",
    "        )\n",
    "\n",
    "        if best_fitness_epoch is not None:\n",
    "            comparison_metrics[model_name] = {\n",
    "                \"best_fitness_epoch\": best_fitness_epoch,\n",
    "                \"best_fitness\": best_fitness_value,\n",
    "                \"mAP50_at_best_fitness\": mAP50_at_best_fitness,\n",
    "                \"mAP50-95_at_best_fitness\": mAP50_95_at_best_fitness,\n",
    "            }\n",
    "    else:\n",
    "        logger.warning(\n",
    "            f\"No analysis data available for {model_name} to include in comparison.\"\n",
    "        )\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "if comparison_metrics:\n",
    "    comparison_df = pd.DataFrame.from_dict(comparison_metrics, orient=\"index\")\n",
    "    # Sort by best fitness (descending)\n",
    "    comparison_df = comparison_df.sort_values(by=\"best_fitness\", ascending=False)\n",
    "\n",
    "    print(\"\\n--- Model Comparison Based on Best Fitness Epoch ---\")\n",
    "    print(comparison_df)\n",
    "    print(\"----------------------------------------------------\")\n",
    "else:\n",
    "    print(\"\\nNo valid analysis results found for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Learning Curves\n",
    "\n",
    "models_to_plot = list(results_data.keys())[-3:]  # Show last 3 models\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# mAP50 plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for model_name in models_to_plot:\n",
    "    df = results_data.get(model_name, {}).get(\"df\")\n",
    "    if df is not None and MAP50_COL in df.columns:\n",
    "        plt.plot(df[\"epoch\"], df[MAP50_COL], label=f\"{model_name} mAP50\")\n",
    "plt.title(\"Validation mAP@0.50 vs Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"mAP@0.50\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# mAP50-95 plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for model_name in models_to_plot:\n",
    "    df = results_data.get(model_name, {}).get(\"df\")\n",
    "    if df is not None and MAP50_95_COL in df.columns:\n",
    "        plt.plot(df[\"epoch\"], df[MAP50_95_COL], label=f\"{model_name} mAP50-95\")\n",
    "plt.title(\"Validation mAP@0.50-0.95 vs Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"mAP@0.50-0.95\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
